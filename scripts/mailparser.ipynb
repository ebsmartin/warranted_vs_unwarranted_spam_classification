{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailparser\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory to search\n",
    "directory = 'C:\\\\Users\\\\ericb\\\\Desktop\\\\Research\\\\Primary@gmail.com\\\\Cleaned_Mail\\\\2023_test\\\\08\\\\'\n",
    "output_csv = 'C:\\\\Users\\\\ericb\\\\Desktop\\\\Research\\\\542_Project\\\\data\\\\test\\\\warranted_data_test_output\\\\mailparser_test_output.csv'\n",
    "\n",
    "# DataFrame to store the results\n",
    "df = pd.DataFrame(columns=['filename', 'body', 'subject', 'text_plain', 'text_html', 'text_not_managed', 'defects', 'defects_categories'])\n",
    "\n",
    "print('Searching files in directory: ' + directory)\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):  # Check for .eml files\n",
    "        print('Processing file: ' + filename)\n",
    "\n",
    "        # Parse the email\n",
    "        mail = mailparser.parse_from_file(os.path.join(directory, filename))\n",
    "\n",
    "        # mail = mailparser.parse_from_bytes(file)\n",
    "        # mail = mailparser.parse_from_file(file)\n",
    "        # mail = mailparser.parse_from_file_msg(file)\n",
    "        # mail = mailparser.parse_from_file_obj(file)\n",
    "        # mail = mailparser.parse_from_string(file)\n",
    "\n",
    "        # print(mail.attachments) # list of all attachments\n",
    "        # print(mail.body)\n",
    "\n",
    "        # print(mail.date) # datetime object in UTC\n",
    "        # print(mail.defects) # defect RFC not compliance\n",
    "        # print(mail.defects_categories)  # only defects categories\n",
    "        # print(mail.delivered_to)\n",
    "        # print(mail.from_)\n",
    "        # print(mail.headers) # dict of all headers\n",
    "        # print(mail.mail) # tokenized mail in a object\n",
    "        # print(mail.message) # email.message.Message object\n",
    "        # print(mail.message_as_string) # message as string\n",
    "        # print(mail.message_id)\n",
    "        # print(mail.received)\n",
    "        # print(mail.subject)\n",
    "        # print(mail.text_plain) # only text plain mail parts in a list\n",
    "        # print(mail.text_html) # only text html mail parts in a list\n",
    "        # print(mail.text_not_managed) # all not managed text (check the warning logs to find content subtype)\n",
    "        # print(mail.to)\n",
    "        # print(mail.to_domains)\n",
    "        # print(mail.timezone) # returns the timezone, offset from UTC\n",
    "        # print(mail.mail_partial) # returns only the mains parts of emails\n",
    "\n",
    "        # Collect data and append to DataFrame\n",
    "        df = df.append({\n",
    "            'filename': filename, \n",
    "            'body': mail.body, \n",
    "            'subject': mail.subject, \n",
    "            'text_plain': mail.text_plain, \n",
    "            'text_html': mail.text_html, \n",
    "            'text_not_managed': mail.text_not_managed, \n",
    "            'defects': str(mail.defects),  # Convert list to string\n",
    "            'defects_categories': str(mail.defects_categories)  # Convert list to string\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import email\n",
    "import quopri\n",
    "from bs4 import BeautifulSoup\n",
    "import mailparser\n",
    "\n",
    "def decode_payload(payload):\n",
    "    if payload is None:\n",
    "        return None, 0\n",
    "\n",
    "    encodings = ['utf-8', 'cp437', 'ISO-8859-1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            decoded_text = quopri.decodestring(payload).decode(encoding, errors='replace')\n",
    "            return decoded_text, decoded_text.count('ï¿½')\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    return None, 0\n",
    "\n",
    "def extract_info_from_email(file_path):\n",
    "    # Parsing the email using mailparser\n",
    "    mail = mailparser.parse_from_file(file_path)\n",
    "\n",
    "    # Additional processing as in Script 1\n",
    "    msg = email.message_from_string(mail.message_as_string)\n",
    "    body = ''\n",
    "    unsubscribe_links = []\n",
    "    unknown_chars_count = 0\n",
    "    soup = None  # Initialize soup here\n",
    "\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            part_body, unknown_count = decode_payload(part.get_payload(decode=True))\n",
    "            body += part_body or ''\n",
    "            unknown_chars_count += unknown_count\n",
    "            if part_body and part_body.strip():\n",
    "                part_soup = BeautifulSoup(part_body, 'html5lib')\n",
    "                unsubscribe_links.extend([link['href'] for link in part_soup.find_all('a', href=True) if \"unsubscribe\" in link.text.lower()])\n",
    "                if soup is None:\n",
    "                    soup = part_soup\n",
    "    else:\n",
    "        body, unknown_chars_count = decode_payload(mail.body)\n",
    "\n",
    "    tracking_pixel = len(soup.find_all('img', width='1', height='1')) > 0 if soup else False\n",
    "    total_links = len(soup.find_all('a')) if soup else 0\n",
    "\n",
    "    dkim_signature = 'Present' if msg.get('DKIM-Signature') else 'Absent'\n",
    "\n",
    "    return {\n",
    "        'filename': os.path.basename(file_path),\n",
    "        'body': body,\n",
    "        'subject': mail.subject,\n",
    "        'text_plain': mail.text_plain,\n",
    "        'text_html': mail.text_html,\n",
    "        'text_not_managed': mail.text_not_managed,\n",
    "        'defects': str(mail.defects),\n",
    "        'defects_categories': str(mail.defects_categories),\n",
    "        'number of unsubscribe links': len(unsubscribe_links),\n",
    "        'number of undecodable characters': unknown_chars_count,\n",
    "        'tracking pixel present': tracking_pixel,\n",
    "        'total links in email': total_links,\n",
    "        'email size (bytes)': len(body),\n",
    "        'dkim-signature': dkim_signature\n",
    "    }\n",
    "\n",
    "# Directory to search\n",
    "directory = 'C:\\\\Users\\\\ericb\\\\Desktop\\\\Research\\\\Primary@gmail.com\\\\Cleaned_Mail\\\\2023_test\\\\08\\\\'\n",
    "output_csv = 'C:\\\\Users\\\\ericb\\\\Desktop\\\\Research\\\\542_Project\\\\data\\\\test\\\\warranted_data_test_output\\\\combined_output.csv'\n",
    "\n",
    "infos = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        info = extract_info_from_email(file_path)\n",
    "        infos.append(info)\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "if infos:\n",
    "    df = pd.DataFrame(infos)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that I have the data, I can clean the textual data for use in the model\n",
    "# print(df['body'][0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CSV file and read it into a DataFrame\n",
    "df_to_clean = pd.read_csv(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import quopri\n",
    "import base64\n",
    "\n",
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text, delimiters=(\"\", \"\"))\n",
    "\n",
    "def replace_urls_based_on_context(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        href = a_tag.get('href', '')\n",
    "        url_type = 'UNSAFE_' if href.startswith('http://') else ''\n",
    "        if a_tag.img:\n",
    "            a_tag.string = f'{url_type}IMAGE_URL'\n",
    "        elif isinstance(a_tag.next, NavigableString) and a_tag.next.strip():\n",
    "            a_tag.string = f'{url_type}LINK_URL'\n",
    "        else:\n",
    "            a_tag.string = f'{url_type}BUTTON_URL'\n",
    "    return str(soup)\n",
    "\n",
    "def replace_urls_in_text(text):\n",
    "    http_url_pattern = re.compile(r'http://\\S+')\n",
    "    text = http_url_pattern.sub('UNSAFE_LINK_URL', text)\n",
    "    https_url_pattern = re.compile(r'https://\\S+')\n",
    "    text = https_url_pattern.sub('LINK_URL', text)\n",
    "    return text\n",
    "\n",
    "def decode_quoted_printable(input_data):\n",
    "    if isinstance(input_data, bytes):\n",
    "        return quopri.decodestring(input_data).decode('utf-8', errors='replace')\n",
    "    else:\n",
    "        return quopri.decodestring(input_data.encode()).decode('utf-8', errors='replace')\n",
    "\n",
    "def decode_base64(text):\n",
    "    return base64.b64decode(text).decode('utf-8', errors='replace')\n",
    "\n",
    "def clean_text(raw_text):\n",
    "\n",
    "    #Remove line breaks and continuation equals signs\n",
    "    raw_text = re.sub(r'=\\n', '', raw_text)\n",
    "    # Decode any quoted-printable text\n",
    "    raw_text = quopri.decodestring(raw_text.encode()).decode('utf-8', errors='replace')\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(raw_text, 'lxml')\n",
    "    \n",
    "    # Remove style and script tags and their content\n",
    "    for tag in soup(['style', 'script', 'img']):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Replace URLs in 'a' tags\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        href = a_tag.get('href', '')\n",
    "        url_type = 'UNSAFE ' if href.startswith('http://') else ''\n",
    "        if a_tag.img:\n",
    "            a_tag.string = f'{url_type}IMAGE URL'\n",
    "        elif isinstance(a_tag.next, NavigableString) and a_tag.next.strip():\n",
    "            a_tag.string = f'{url_type}LINK URL'\n",
    "        else:\n",
    "            a_tag.string = f'{url_type}BUTTON URL'\n",
    "\n",
    "    # Now proceed with extracting text and further cleaning\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    text = replace_emojis(text)\n",
    "\n",
    "    text = replace_urls_in_text(text)\n",
    "\n",
    "    # Remove any remaining HTML encoded characters\n",
    "    text = re.sub(r'&[a-zA-Z0-9#]+;', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove Zero Width Non-Joiner characters\n",
    "    text = text.replace('\\u200b', '')  # Unicode for ZWNBSP\n",
    "    text = text.replace('\\u200c', '')  # Unicode for ZWNJ\n",
    "    text = text.replace('\\u200d', '')  # Unicode for ZWJ\n",
    "    text = text.replace('\\u200e', '')  # Unicode for LEFT-TO-RIGHT MARK\n",
    "    text = text.replace('\\u200f', '')  # Unicode for RIGHT-TO-LEFT MARK\n",
    "\n",
    "\n",
    "    # Strip string of leading/trailing whitespace\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_to_clean['body'][1])\n",
    "\n",
    "# run the clean_text function on the subject and body columns\n",
    "df_to_clean['subject'] = df_to_clean['subject'].apply(clean_text)\n",
    "df_to_clean['body'] = df_to_clean['body'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_to_clean['body'][1])\n",
    "\n",
    "# save df_to_clean['body'][i] to txt file\n",
    "for i in range (0, len(df_to_clean['body'])):\n",
    "    with open('C:\\\\Users\\\\ericb\\\\Desktop\\\\Research\\\\542_Project\\\\data\\\\test\\\\warranted_data_test_output\\\\mailparser_test_output_body' + str(i) + '.txt', 'w') as f:\n",
    "        f.write(df_to_clean['body'][i])\n",
    "\n",
    "# save df_to_clean['text_not_managed'][i] to txt file\n",
    "for i in range(0, len(df_to_clean['text_not_managed'])):\n",
    "    with open('C:\\\\Users\\\\ericb\\\\Desktop\\\\Research\\\\542_Project\\\\data\\\\test\\\\warranted_data_test_output\\\\mailparser_test_output_text_not_managed' + str(i) + '.txt', 'w') as f:\n",
    "        f.write(df_to_clean['text_not_managed'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning_20220719",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
