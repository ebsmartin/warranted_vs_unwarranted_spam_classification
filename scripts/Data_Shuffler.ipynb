{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle the data\n",
    "\n",
    "1) Load all CSV files from both datasets into separate DataFrames.\n",
    "2) Shuffle each DataFrame individually.\n",
    "3) Calculate the number of unwarranted rows to distribute per new CSV.\n",
    "4) Create new CSV files by iteratively taking slices from the shuffled DataFrames.\n",
    "5) Ensure that each new CSV has a proportional amount of warranted and unwarranted data.\n",
    "6) Save the new CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericb\\anaconda3\\envs\\machinelearning_20220719\\lib\\site-packages\\ipykernel_launcher.py:13: DtypeWarning: Columns (1,2,5,8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved shuffled and mixed data to \"C:\\Users\\ericb\\Desktop\\Research\\542_Project\\train_test_data\\mixed_data_1.csv\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "warranted_dir = 'C:\\\\Users\\\\ericb\\\\Desktop\\\\Research\\\\542_Project\\\\cleaned_data\\\\warranted_data\\\\'\n",
    "unwarranted_dir = 'C:\\\\Users\\\\ericb\\\\Desktop\\\\Research\\\\542_Project\\\\cleaned_data\\\\unwarranted_data\\\\'\n",
    "output_dir = 'C:\\\\Users\\\\ericb\\\\Desktop\\\\Research\\\\542_Project\\\\train_test_data\\\\'\n",
    "\n",
    "# Function to load and shuffle CSVs from a directory into a single DataFrame\n",
    "def load_and_shuffle_csvs(directory):\n",
    "    all_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    df_list = [pd.read_csv(file) for file in all_files]\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    return shuffle(combined_df)\n",
    "\n",
    "# Load and shuffle warranted and unwarranted datasets\n",
    "warranted_df = load_and_shuffle_csvs(warranted_dir)\n",
    "unwarranted_df = load_and_shuffle_csvs(unwarranted_dir)\n",
    "\n",
    "# Calculate the number of unwarranted rows to distribute per new CSV\n",
    "num_output_csvs = 1\n",
    "unwarranted_per_csv = len(unwarranted_df) // num_output_csvs\n",
    "\n",
    "# Create and save new CSV files with mixed data\n",
    "for i in range(num_output_csvs):\n",
    "    # Slice the warranted DataFrame for this CSV\n",
    "    warranted_slice = warranted_df[i*unwarranted_per_csv:(i+1)*unwarranted_per_csv]\n",
    "    \n",
    "    # Slice the unwarranted DataFrame, wrap around if we reach the end\n",
    "    start_idx = i * unwarranted_per_csv\n",
    "    end_idx = start_idx + unwarranted_per_csv\n",
    "    if end_idx > len(unwarranted_df):\n",
    "        end_idx -= len(unwarranted_df)\n",
    "        unwarranted_slice = pd.concat([unwarranted_df[start_idx:], unwarranted_df[:end_idx]])\n",
    "    else:\n",
    "        unwarranted_slice = unwarranted_df[start_idx:end_idx]\n",
    "    \n",
    "    # Combine the slices and shuffle\n",
    "    mixed_df = shuffle(pd.concat([warranted_slice, unwarranted_slice], ignore_index=True))\n",
    "    \n",
    "    # Save to a new CSV file\n",
    "    output_file = os.path.join(output_dir, f'mixed_data_{i+1}.csv')\n",
    "    mixed_df.to_csv(output_file, index=False)\n",
    "    print(f'Saved shuffled and mixed data to \"{output_file}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from mixed_data_1.csv split into training and test sets and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Directory containing the data\n",
    "dir = 'C:\\\\Users\\\\ericb\\\\Desktop\\\\Research\\\\542_Project\\\\train_test_data\\\\'\n",
    "\n",
    "# Percentage of data to be used for testing (e.g., 0.2 for 20%)\n",
    "test_size = 0.2\n",
    "\n",
    "# Iterate over each CSV file in the directory\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        train_df, test_df = train_test_split(df, test_size=test_size)\n",
    "\n",
    "        # Save the subsets to new CSV files\n",
    "        train_df.to_csv(os.path.join(dir, f'train_{filename}'), index=False)\n",
    "        test_df.to_csv(os.path.join(dir, f'test_{filename}'), index=False)\n",
    "\n",
    "        print(f\"Data from {filename} split into training and test sets and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python10_ML_NLP_Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
